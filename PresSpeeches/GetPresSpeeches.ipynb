{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCCB Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isExist = os.path.exists(\"SpeechesTR\\\\Turkish\")\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(\"SpeechesTR\\\\Turkish\")\n",
    "   os.makedirs(\"SpeechesTR\\\\English\")\n",
    "\n",
    "df = pd.DataFrame(columns = [\"date\", \"title\", \"file\", \"link\"])\n",
    "count = 1\n",
    "\n",
    "for page_num in range(1, 33):\n",
    "    if (page_num == 1):\n",
    "        url = \"https://www.tccb.gov.tr/receptayyiperdogan/konusmalar/\"\n",
    "    else:\n",
    "        url = \"https://www.tccb.gov.tr/receptayyiperdogan/konusmalar/?&page=\" + str(page_num)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    list_press_address = soup.find(\"div\", {\"id\": \"list-press-address\"})\n",
    "    dl_data = list_press_address.find_all(\"dl\")\n",
    "    for dlitem in dl_data:\n",
    "        speech_date = dlitem.find_all(\"dt\")[0].text\n",
    "        speech_title = dlitem.find_all(\"dd\")[0].text\n",
    "        speech_url = \"https://www.tccb.gov.tr\" + dlitem.find_all(\"a\")[0].get(\"href\")\n",
    "        speech_page = requests.get(speech_url)\n",
    "        speech_soup = BeautifulSoup(speech_page.text, 'lxml')\n",
    "        speech_by_p = speech_soup.find(\"div\", {\"id\": \"divContentArea\"})\n",
    "        if (speech_by_p is None):\n",
    "            print(speech_title)\n",
    "            continue\n",
    "        speech_text = \"\"\n",
    "        for p in speech_by_p.find_all(\"p\"):\n",
    "            speech_text += p.text + \" \"\n",
    "        \n",
    "        file_name = \"PresSpeech\" + str(count) + \".txt\"\n",
    "        with open(\"SpeechesTR\\\\Turkish\\\\\" + file_name, 'w', encoding='mbcs') as f:\n",
    "            f.write(speech_text)\n",
    "        to_append = [speech_date, speech_title, file_name, speech_url]\n",
    "        count += 1\n",
    "\n",
    "        a_series = pd.Series(to_append, index = df.columns)\n",
    "        df = df.append(a_series, ignore_index=True)\n",
    "\n",
    "    print(page_num, \"COMPLETED\")\n",
    "    sleep(2)\n",
    "    df.to_csv(\"PresSpeechesTR.csv\", index=False)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PresSpeechesTR.csv\")\n",
    "print(df.shape[0])\n",
    "sum = 0\n",
    "count = 0\n",
    "max = 0\n",
    "min = 100000\n",
    "for index, row in df.iterrows():\n",
    "    text_file = open(df[index, \"file\"], \"r\", encoding=\"mbcs\")\n",
    "    speech = (text_file.read())\n",
    "    count += 1\n",
    "    speech_length = len(speech)\n",
    "    sum += speech_length\n",
    "    if (speech_length > max):\n",
    "        max = speech_length\n",
    "    if (speech_length < min):\n",
    "        min = speech_length\n",
    "print(\"Average character count\", sum/count)\n",
    "print(\"Max character count\", min)\n",
    "print(\"Min character count\", max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import storage\n",
    "from nltk import tokenize\n",
    "\n",
    "def translate_text(target, text):\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    import six\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client.from_service_account_json(\n",
    "        \"universal-sun-342204-0d31a6d61733.json\")\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target, source_language=\"tr\")\n",
    "    # print(u\"Text: {}\".format(result[\"input\"]))\n",
    "    # print(u\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    # print(u\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "    # print(result[\"translatedText\"])\n",
    "    return result[\"translatedText\"]\n",
    "\n",
    "df = pd.read_csv(\"PresSpeechesTR.csv\")\n",
    "df.insert(2, \"translated_title\", \"\") \n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "for index, row in df.iterrows():\n",
    "    text_file = open(df[index, \"file\"], \"r\", encoding=\"mbcs\")\n",
    "    speech = (text_file.read())\n",
    "    list_sentences = tokenize.sent_tokenize(speech)\n",
    "    translated = \"\"\n",
    "    to_translate = \"\"\n",
    "    for sentence in list_sentences:\n",
    "        if (len(to_translate) + len(sentence) < 5000):\n",
    "            to_translate += sentence + \" \"\n",
    "        else:\n",
    "            translated += translate_text(\"en\", to_translate) + \" \"\n",
    "            to_translate = \"\"\n",
    "    translated += translate_text(\"en\", to_translate) + \" \"\n",
    "    with open(\"SpeechesTR\\\\English\\\\\" + df[index, \"file\"], 'w', encoding='utf-8') as f:\n",
    "        f.write(translated)\n",
    "    df[index, \"translated_title\"] = translate_text(\"en\", row[\"title\"])\n",
    "df.to_csv(\"PresSpeechesTR.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCCB English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"22.08.2022\"\n",
    "# range(1, 17)\n",
    "\n",
    "isExist = os.path.exists(\"SpeechesEN\")\n",
    "if not isExist:\n",
    "   os.makedirs(\"SpeechesEN\")\n",
    "\n",
    "df = pd.DataFrame(columns = [\"date\", \"title\", \"file\", \"link\"])\n",
    "count = 1\n",
    "found = False\n",
    "for page_num in range(1, 17):\n",
    "    if (page_num == 1):\n",
    "        url = \"https://www.tccb.gov.tr/en/receptayyiperdogan/speeches/\"\n",
    "    else:\n",
    "        url = \"https://www.tccb.gov.tr/en/receptayyiperdogan/speeches/?&page=\" + str(page_num)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    list_press_address = soup.find(\"div\", {\"id\": \"list-press-address\"})\n",
    "    dl_data = list_press_address.find_all(\"dl\")\n",
    "    for dlitem in dl_data:\n",
    "        speech_date = dlitem.find_all(\"dt\")[0].text\n",
    "        if speech_date == \"22.08.2022\":\n",
    "            found = True\n",
    "        if not found:\n",
    "            continue\n",
    "        speech_title = dlitem.find_all(\"dd\")[0].text\n",
    "        speech_url = \"https://www.tccb.gov.tr\" + dlitem.find_all(\"a\")[0].get(\"href\")\n",
    "        speech_page = requests.get(speech_url)\n",
    "        speech_soup = BeautifulSoup(speech_page.text, 'lxml')\n",
    "        speech_by_p = speech_soup.find(\"div\", {\"id\": \"divContentArea\"})\n",
    "        if (speech_by_p is None):\n",
    "            print(speech_title)\n",
    "            continue\n",
    "        speech_text = \"\"\n",
    "        for p in speech_by_p.find_all(\"p\"):\n",
    "            speech_text += p.text + \" \"\n",
    "        \n",
    "        file_name = \"PresSpeechEN\" + str(count) + \".txt\"\n",
    "        with open(\"SpeechesEN\\\\\" + file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(speech_text)\n",
    "        to_append = [speech_date, speech_title, file_name, speech_url]\n",
    "        count += 1\n",
    "\n",
    "        a_series = pd.Series(to_append, index = df.columns)\n",
    "        df = df.append(a_series, ignore_index=True)\n",
    "\n",
    "    print(page_num, \"COMPLETED\")\n",
    "    sleep(2)\n",
    "    df.to_csv(\"PresSpeechesEN.csv\", index=False)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Gallery in Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "isExist = os.path.exists(\"Videos\")\n",
    "if not isExist:\n",
    "   os.makedirs(\"Videos\")\n",
    "\n",
    "# count = 1\n",
    "df = pd.DataFrame(columns = [\"title\", \"file\", \"url\"])\n",
    "\n",
    "for i in range(1, 245):\n",
    "    url = \"https://www.tccb.gov.tr/canliyayin/?Keyword=&page=\" + str(i) + \"#Video\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    search_results_div = soup.find(\"div\", {\"id\": \"SearchResult\"})\n",
    "    video_divs = search_results_div.find_all(\"div\", {\"class\": \"video-gallery-area\"})\n",
    "\n",
    "    for video_div in video_divs:\n",
    "        video_spot = video_div.find(\"div\", {\"class\": \"video-spot\"})\n",
    "        video_url = \"https://www.tccb.gov.tr\"\n",
    "        video_url += video_spot.find(\"img\")['src'].replace(\"resim/vposter\", \"video\").replace(\".jpg\", \".mp4\")\n",
    "        # r = requests.get(video_url, stream = True) \n",
    "        # file_name = \"Video\" + str(count) + \".mp4\"\n",
    "        # count += 1\n",
    "        # download started \n",
    "        # with open(\"Videos\\\\\" + file_name, 'wb') as f: \n",
    "        #     for chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "        #         if chunk: \n",
    "        #             f.write(chunk) \n",
    "\n",
    "        file_name = wget.download(video_url, out=\"\\\\Videos\\\\\")\n",
    "        \n",
    "        video_title = video_div.find(\"div\", {\"class\": \"video-title\"}).find(\"h2\").text.strip()\n",
    "        to_append = [video_title, file_name, video_url]\n",
    "        a_series = pd.Series(to_append, index = df.columns)\n",
    "        df = df.append(a_series, ignore_index=True)\n",
    "    print(\"Page\", i, \"completed.\")\n",
    "    df.to_csv(\"PresVideos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10 completed.\n",
      "Page 20 completed.\n",
      "Page 30 completed.\n",
      "Page 40 completed.\n",
      "Page 50 completed.\n",
      "Page 60 completed.\n",
      "Page 70 completed.\n",
      "Page 80 completed.\n",
      "Page 90 completed.\n",
      "Page 100 completed.\n",
      "Page 110 completed.\n",
      "Page 120 completed.\n",
      "Page 130 completed.\n",
      "Page 140 completed.\n",
      "Page 150 completed.\n",
      "Page 160 completed.\n",
      "Page 170 completed.\n",
      "Page 180 completed.\n",
      "Page 190 completed.\n",
      "Page 200 completed.\n",
      "Page 210 completed.\n",
      "Page 220 completed.\n",
      "Page 230 completed.\n",
      "Page 240 completed.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = [\"title\", \"url\", \"date\"])\n",
    "\n",
    "for i in range(1, 245):\n",
    "    url = \"https://www.tccb.gov.tr/canliyayin/?Keyword=&page=\" + str(i) + \"#Video\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    search_results_div = soup.find(\"div\", {\"id\": \"SearchResult\"})\n",
    "    video_divs = search_results_div.find_all(\"div\", {\"class\": \"video-gallery-area\"})\n",
    "\n",
    "    for video_div in video_divs:\n",
    "        video_spot = video_div.find(\"div\", {\"class\": \"video-spot\"})\n",
    "        video_url = \"https://www.tccb.gov.tr\"\n",
    "        video_url += video_spot.find(\"img\")['src'].replace(\"resim/vposter\", \"video\").replace(\".jpg\", \".mp4\")\n",
    "        \n",
    "        video_title = video_div.find(\"div\", {\"class\": \"video-title\"}).find(\"h2\").text.strip()\n",
    "        date = \"\"\n",
    "        date_list = video_url.split(\"/\")[-1].split(\"-\")\n",
    "        if len(date_list) >= 3:\n",
    "            year, month, day = date_list[0], date_list[1], date_list[2]\n",
    "            date = day + \".\" + month + \".\" + year\n",
    "        to_append = [video_title, video_url, date]\n",
    "        a_series = pd.Series(to_append, index = df.columns)\n",
    "        df = df.append(a_series, ignore_index=True)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Page\", i, \"completed.\")\n",
    "        sleep(2)\n",
    "\n",
    "df.to_csv(\"PresVideoLinks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "already_downloaded = os.listdir(\"Videos\")\n",
    "df = pd.read_csv(\"PresVideoLinks.csv\")\n",
    "for url in df[\"url\"]:\n",
    "    if url.split(\"/\")[-1] not in already_downloaded:\n",
    "        file_name = wget.download(url, out= os.getcwd() + \"\\\\Videos\\\\\")\n",
    "        print(file_name)\n",
    "# df[\"url\"][0].split(\"/\")[-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
